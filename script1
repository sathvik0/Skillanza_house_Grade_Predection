# -*- coding: utf-8 -*-
"""
Created on Thu Nov 21 14:41:13 2019

@author: sathvik.inti
"""
import pandas as pd
import os
import numpy as np
import xgboost as xgb
from sklearn.model_selection import GridSearchCV


os.chdir(r'C:\Users\sathvik.inti\Desktop\skillanza_2')
train=pd.read_csv('train.csv')
test=pd.read_csv('test.csv')
sub=pd.read_csv('sub.csv')
train.isna().sum()

#import pandas_profiling
#profile = train.profile_report(title='Pandas Profiling Report')
#profile.to_file(output_file="output.html")

train['Price'] = train['Price'].map(lambda x: x.rstrip('$'))
train['Price']=train['Price'].astype(int)

test['Price'] = test['Price'].map(lambda x: x.rstrip('$'))
test['Price']=test['Price'].astype(int)

train['roof']=train['roof'].str.lower()
test['roof']=test['roof'].str.lower()
Y=train['Grade']
train=train.drop(['id','Grade'],axis=1)
test=test.drop(['id'],axis=1)

def onehot(data):
    cat=data.select_dtypes(include=['object'])
    list_cat=list(cat.columns)
    dataf = pd.concat([data.drop(list_cat, axis=1), pd.get_dummies(cat)], axis=1)
    return dataf

train=onehot(train)
test=onehot(test)

test['Lawn(Area)'].plot(kind='kde')


from missingpy import MissForest
imputer = MissForest()
train1=imputer.fit_transform(train)
test1=imputer.fit_transform(test)

from imblearn.over_sampling import SMOTE
sm = SMOTE(ratio='auto', random_state=None,  k_neighbors=5,  m_neighbors=10, out_step=0.5, kind='regular', svm_estimator=None, n_jobs=1)
X_res, y_res = sm.fit_sample(train1, Y)


from xgboost import XGBClassifier
model_xgb = XGBClassifier(
 tree_method = 'exact',
 learning_rate =0.1,
 n_estimators=500,
 max_depth=5,
 min_child_weight=1,
 gamma=0,
 eval_metric='mlogloss',
 subsample=0.5,
 colsample_bytree=0.8,
 objective= 'multi:softmax',
 scale_pos_weight=51,
 nthread=4,
 seed=52)

xgb2=model_xgb.fit(train1,Y)
preds=xgb2.predict(test1)

xgb1=model_xgb.fit(X_res, y_res)
preds=xgb1.predict(test1)
sub['Grade']=preds
pd.value_counts(sub['Grade'])
sub.to_csv('xgb10.csv',index=False)

#pseudo labeling
predsk=pd.DataFrame(xgb1.predict_proba(test1),columns=['a','b','c','d','e'])
preds1 = predsk.max(axis=1)

test=pd.read_csv('test.csv')
train=pd.read_csv('train.csv')

test['preds_prob']=preds1
test['Grade']=preds

pseudo_test=test[test['preds_prob']>0.98]
pseudo_test=pseudo_test.drop(['preds_prob'],axis=1)
train=train.append(pseudo_test)

train['Price'] = train['Price'].map(lambda x: x.rstrip('$'))
train['Price']=train['Price'].astype(int)

test=pd.read_csv('test.csv')
test['Price'] = test['Price'].map(lambda x: x.rstrip('$'))
test['Price']=test['Price'].astype(int)

train['roof']=train['roof'].str.lower()
test['roof']=test['roof'].str.lower()
Y=train['Grade']
train=train.drop(['id','Grade'],axis=1)
test=test.drop(['id'],axis=1)

def onehot(data):
    cat=data.select_dtypes(include=['object'])
    list_cat=list(cat.columns)
    dataf = pd.concat([data.drop(list_cat, axis=1), pd.get_dummies(cat)], axis=1)
    return dataf

train=onehot(train)
test=onehot(test)

from missingpy import MissForest
imputer = MissForest()
train1=imputer.fit_transform(train)
test1=imputer.fit_transform(test)

from xgboost import XGBClassifier
model_xgb = XGBClassifier(
        tree_method = 'exact',
 learning_rate =0.1,
 n_estimators=500,
 max_depth=2,
 min_child_weight=1,
 gamma=0,
 eval_metric='mlogloss',
 subsample=0.7,
 colsample_bytree=0.8,
 objective= 'multi:softmax',
 scale_pos_weight=42,
 nthread=4,
 seed=42)


xgb1=model_xgb.fit(train1, Y)
preds=xgb1.predict(test1)
sub['Grade']=preds
sub.to_csv('xgb8.csv',index=False)
